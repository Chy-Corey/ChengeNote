# YOLOv3

## 一、YOLOv3中的三个基本组件

1. **CBL：**Yolov3网络结构中的最小组件，由**Conv+Bn+Leaky_relu**激活函数三者组成。
2. **Res unit：**借鉴**Resnet**网络中的残差结构，让网络可以构建的更深。
3. **ResX：**由一个**CBL**和**X**个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是**608->304->152->76->38->19大小**。

---

### CBL

#### 1. Convolutional (卷积层)

- 卷积层的定义

  容易出现的误区：一个卷积层并不一定只有一层，它与输入图像的层数是相同的，如果这个图像有RGB三层，那么卷积核也是三层。但是求出来的输出值只有一层。比如输入图像为32 * 32 * 3，用5 * 5 * 3的卷积核以步长为1进行卷积，得到的结果为 28 * 28 * 1。

  如果有多个卷积核对同一个输入进行卷积，那么它们的输出会堆叠起来，构成层数。比如用两个不同的5 * 5 * 3的卷积核，对一张32 * 32 * 3的图像进行卷积，得到结果为28 * 28 * 2。

- 卷积层的作用

  基本上，如果输入图像中有与卷积核代表的形状很相似的图形，那么所有乘积的和会很大。但是如果感知域中没有与卷积核表示的相一致的形状，得到的值就会小很多。

  卷积层的作用是输出一张激活图。假设卷积核是一个曲线识别器，那么所得的激活图会显示出哪些地方最有可能有曲线。卷积核越多，激活图的深度就越深，我们得到的关于输入图像的信息就越多。

- 卷积层的叠加

  我们会发现，在网络中会有多层卷积层。因为随着卷积层数的增加，可以获得更上层的特征。比如第一层卷积只能识别直线和曲线，接着第二层可以识别三角形、矩形和原型，再向上就可以识别具体的物体了。

#### 2. Fully connected layer(全连接层)

全连接层用于最后的特征归纳，并进行分类。比如现在这个模型，用于分类猫和狗。我们进行卷积和池化，得到了四个特征：A B C D 。其中：A B 是小猫的特征，C D 是小狗的特征。因为是分类小猫小狗，所以全连接层有两个值：M N。现在将最后一层的特征值和全连接层进行全连接：

M = a1 * A + b1 * B + c1 * C + d1 * D;

N = a2 * A + b2 * B + c2 * C + d2 * D;

因为AB表示小猫，CD表示小狗，所以 a1, b1, c2, d2 要比 a2, b2, c1, d1 更大一些。

全连接层就是把求到的特征都连接到一起，进行最后的分类。

#### 3. Max pool(池化层)

- 池化的作用：

​	1. 保留主要特征的同时减少参数和计算量，防止过拟合。

​	2. invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算	速度的不断提高，我认为这个妥协会越来越小。

现在有些网络都开始少用或者不用pooling层了。

#### 4. Batchnorm(批量归一化)

深度学习需要对数据做归一化，因为深度神经网络主要就是为了学习训练数据的分布，并在测试集上达到很好的泛化效果，但是，如果我们每一个batch输入的数据都具有不同的分布，显然会给网络的训练带来困难。另一方面，数据经过一层层网络计算后，其数据分布也在发生着变化，此现象称为Internal Covariate Shift。

Batch Norm 层是对每层数据归一化后再进行线性变换改善数据分布, 其中的线性变换是可学习的。

一般先对数据进行归一化，然后进行激活。

#### 5. Leaky_ReLU(leaky线性整流函数)

是一种激活函数，可以防止神经元死亡(通常，激活函数的输入值有一项偏置项(bias)，假设bias变得太小，以至于输入激活函数的值总是负的，那么反向传播过程经过该处的梯度恒为0,对应的权重和偏置参数此次无法得到更新)。



### Res Unit

